#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
01_sac_pipeline.py â€” å®ç° SAC (Summary-Augmented Chunking)
åŠŸèƒ½ï¼š
 1. è¯»å– PDF å…¨æ–‡
 2. è°ƒç”¨ LLM ç”Ÿæˆæ–‡æ¡£çº§æ‘˜è¦ (Document Fingerprint)
 3. å¯¹å…¨æ–‡è¿›è¡Œåˆ‡ç‰‡ (Chunking)
 4. æ‹¼æ¥: Augmented Text = Summary + "\n\n" + Chunk Body
 5. ä¿å­˜ç»“æœç”¨äºåµŒå…¥
"""

import os
import json
import time
import re
from pathlib import Path
import fitz  # PyMuPDF
from openai import OpenAI
import httpx
from httpx_socks import SyncProxyTransport

# ====== é…ç½® ======
RAW_ROOT = Path("raw")
OUTPUT_DIR = Path("cleaned_sac")  # æ–°çš„è¾“å‡ºç›®å½•ï¼ŒåŒºåˆ†äºæ™®é€šçš„ cleaned
OUTPUT_DIR.mkdir(exist_ok=True)

JURISDICTIONS = ["eu", "us", "sg", "cn", "br", "sv", "jp", "uk", "hk", "kr", "ch", "uae"]

# [cite_start]SAC å‚æ•° [cite: 160-165, 360-380]
SUMMARY_CHAR_LIMIT = 150  # è®ºæ–‡å»ºè®®æ‘˜è¦é•¿åº¦
CHUNK_SIZE = 500          # åˆ‡ç‰‡å¤§å°
CHUNK_OVERLAP = 0         # è®ºæ–‡ä¸­è®¾ç½®ä¸ºæ— é‡å 

# LLM é…ç½®
GPTS_BASE_URL = os.getenv("GPTS_BASE_URL", "https://api.gptsapi.net/v1")
API_KEY = os.getenv("GPTSAPI_API_KEY")
PROXY_URL = os.getenv("PROXY_URL", "") # ç•™ç©ºåˆ™ä¸ä½¿ç”¨ä»£ç†
SUMMARY_MODEL = "gpt-4o-mini" # ç”¨ä¾¿å®œçš„æ¨¡å‹ç”Ÿæˆæ‘˜è¦å³å¯

# ====== å·¥å…·å‡½æ•° ======

def create_client():
    if not API_KEY:
        raise ValueError("âŒ ç¼ºå°‘ GPTSAPI_API_KEY ç¯å¢ƒå˜é‡ï¼")
    
    if PROXY_URL and PROXY_URL.strip() != "":
        print(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {PROXY_URL}")
        transport = SyncProxyTransport.from_url(PROXY_URL)
        http_client = httpx.Client(transport=transport, timeout=60.0)
        return OpenAI(api_key=API_KEY, base_url=GPTS_BASE_URL, http_client=http_client)
    else:
        return OpenAI(api_key=API_KEY, base_url=GPTS_BASE_URL)

def normalize_whitespace(text: str) -> str:
    if not text: return ""
    text = text.replace("\u00A0", " ")
    text = re.sub(r"-\n(?=[a-z])", "", text)
    text = re.sub(r"\r\n?", "\n", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def extract_full_text(pdf_path: Path) -> str:
    """æå–æ•´ä¸ª PDF çš„æ–‡æœ¬ç”¨äºæ€»ç»“"""
    doc = fitz.open(str(pdf_path))
    full_text = []
    # ä¸ºäº†çœé’±/çœæ—¶é—´ï¼Œå¦‚æœæ–‡ä»¶å·¨å¤§ï¼ˆè¶…è¿‡50é¡µï¼‰ï¼Œåªå–å‰10é¡µå’Œå5é¡µç”Ÿæˆæ‘˜è¦
    # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æå–å…¨æ–‡ï¼ˆå› ä¸ºæ³•å¾‹æ–‡ä»¶é€šå¸¸éœ€è¦é€šè¯»ï¼‰
    for page in doc:
        full_text.append(page.get_text())
    return normalize_whitespace("\n".join(full_text))

def generate_document_summary(client, text: str, doc_name: str) -> str:
    """[Goal 1] è°ƒç”¨ LLM ç”Ÿæˆæ–‡æ¡£æ‘˜è¦ (Document Fingerprint)"""
    print(f"ğŸ¤– æ­£åœ¨ä¸º {doc_name} ç”Ÿæˆæ‘˜è¦...")
    
    # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬é˜²æ­¢ Token æº¢å‡º (ä¾‹å¦‚åªå–å‰ 15000 å­—ç¬¦ç”¨äºæ‘˜è¦)
    # å®é™…ç”Ÿäº§ä¸­å¯ä»¥ä½¿ç”¨ Map-Reduce æ‘˜è¦ï¼Œä½†è¿™é‡Œç®€åŒ–å¤„ç†
    context_text = text[:15000] 

    prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹æ³•å¾‹æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªæç®€çš„â€˜æ–‡æ¡£æŒ‡çº¹â€™ï¼ˆæ‘˜è¦ï¼‰ã€‚\n"
        f"è¦æ±‚ï¼š\n"
        f"1. åŒ…å«æ ¸å¿ƒæ³•å¾‹ä¸»é¢˜ã€é€‚ç”¨èŒƒå›´å’Œå…³é”®å®ä½“ã€‚\n"
        f"2. é•¿åº¦ä¸¥æ ¼æ§åˆ¶åœ¨ {SUMMARY_CHAR_LIMIT} ä¸ªå­—ç¬¦å·¦å³ã€‚\n"
        f"3. ä¸è¦åºŸè¯ï¼Œç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ã€‚\n\n"
        f"æ–‡æ¡£å†…å®¹æ‘˜è¦ï¼ˆæˆªå–ï¼‰ï¼š\n{context_text}..."
    )

    try:
        resp = client.chat.completions.create(
            model=SUMMARY_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=200
        )
        summary = resp.choices[0].message.content.strip()
        print(f"âœ… æ‘˜è¦ç”Ÿæˆ: {summary}")
        return summary
    except Exception as e:
        print(f"âš ï¸ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ–‡ä»¶åä»£æ›¿ã€‚")
        return f"Document: {doc_name}"

def recursive_chunk_text(text: str, chunk_size=500, overlap=0):
    """ç®€å•çš„åˆ‡ç‰‡é€»è¾‘"""
    chunks = []
    start = 0
    text_len = len(text)
    
    while start < text_len:
        end = min(start + chunk_size, text_len)
        # ç®€å•ä¼˜åŒ–ï¼šå°è¯•åœ¨æ¢è¡Œæˆ–å¥å·å¤„æˆªæ–­
        if end < text_len:
            lookback = text.rfind('\n', start, end)
            if lookback != -1 and lookback > start + chunk_size * 0.8:
                end = lookback + 1
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        start = end - overlap
    return chunks

# ====== ä¸»æµç¨‹ ======

def process_file_sac(client, pdf_path: Path):
    doc_id = pdf_path.stem
    jurisdiction = pdf_path.parent.name.upper()
    
    # 1. æå–å…¨æ–‡
    full_text = extract_full_text(pdf_path)
    if len(full_text) < 100:
        print(f"â­ï¸ è·³è¿‡ {doc_id} (å†…å®¹å¤ªå°‘ï¼Œå¯èƒ½æ˜¯æ‰«æä»¶)")
        return

    # 2. ç”Ÿæˆæ‘˜è¦ (SAC æ ¸å¿ƒæ­¥éª¤)
    doc_summary = generate_document_summary(client, full_text, doc_id)

    # 3. åˆ‡åˆ†
    raw_chunks = recursive_chunk_text(full_text, CHUNK_SIZE, CHUNK_OVERLAP)

    # 4. æ„å»º SAC æ•°æ®å¹¶å†™å…¥
    out_file = OUTPUT_DIR / f"{doc_id}.jsonl"
    with open(out_file, "w", encoding="utf-8") as f:
        for idx, chunk_body in enumerate(raw_chunks):
            # [SAC æ ¸å¿ƒ] æ‹¼æ¥æ‘˜è¦ + åŸå§‹å†…å®¹
            augmented_text = f"Doc Summary: {doc_summary}\n\nContent: {chunk_body}"
            
            record = {
                "chunk_id": f"{doc_id}_{idx}",
                "doc_id": doc_id,
                "jurisdiction": jurisdiction,
                "text": chunk_body,         # åŸå§‹æ–‡æœ¬ (ç”¨äºæ˜¾ç¤º)
                "augmented_text": augmented_text, # ç”¨äºåµŒå…¥å‘é‡çš„æ–‡æœ¬ï¼
                "summary": doc_summary,     # å­˜å‚¨æ‘˜è¦ä»¥å¤‡æŸ¥
                "chunk_index": idx
            }
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    
    print(f"ğŸ’¾ å·²ä¿å­˜ {len(raw_chunks)} ä¸ª SAC åˆ‡ç‰‡åˆ° {out_file}")

def main():
    print("ğŸš€ å¯åŠ¨ SAC (Summary-Augmented Chunking) æµæ°´çº¿...")
    client = create_client()
    
    for jur in JURISDICTIONS:
        folder = RAW_ROOT / jur
        if not folder.exists(): continue
        
        pdfs = sorted(folder.glob("*.pdf"))
        print(f"\nğŸ“‚ å¤„ç†æ³•åŸŸ: {jur} ({len(pdfs)} æ–‡ä»¶)")
        
        for pdf in pdfs:
            process_file_sac(client, pdf)
            time.sleep(1) # é¿å… API é€Ÿç‡é™åˆ¶

if __name__ == "__main__":
    main()
