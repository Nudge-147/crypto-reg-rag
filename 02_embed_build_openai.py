#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŠŸèƒ½ï¼šè¯»å– cleaned_sac/ ä¸‹çš„ SAC æ³•è§„ chunk æ•°æ® â†’ ç”¨ GPTsAPI ç”Ÿæˆå‘é‡ â†’ æ„å»º FAISS ç´¢å¼•  
ä½¿ç”¨è¯´æ˜ï¼š
 1. ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ– (pip install openai httpx httpx-socks faiss-cpu numpy tqdm tiktoken)
 2. export GPTSAPI_API_KEY="ä½ çš„ GPTsAPI å¯†é’¥"
 3. åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼š python3 02_embed_build_openai.py
"""

import os
import json
import time
from pathlib import Path
from typing import Optional

import numpy as np
import faiss
from tqdm import tqdm

from openai import OpenAI
import openai
import httpx
# åªæœ‰è®¾ç½®äº† PROXY_URL ç¯å¢ƒå˜é‡æ‰éœ€è¦ httpx_socks
try:
    from httpx_socks import SyncProxyTransport
except ImportError:
    SyncProxyTransport = None
    
import tiktoken  # ç”¨äº token ä¼°ç®—

# ====== é…ç½®å‚æ•° ======
# ç¡®ä¿æŒ‡å‘ SAC æµç¨‹çš„è¾“å‡ºç›®å½•
CLEANED_DIR = Path("cleaned_sac") 

INDEX_DIR = Path("indexes")
INDEX_PATH = INDEX_DIR / "faiss.index"
META_PATH = INDEX_DIR / "meta.jsonl"

PROXY_URL = os.getenv("PROXY_URL", "") # ç•™ç©ºåˆ™ä¸ä½¿ç”¨ä»£ç†

EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-large")
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "16"))
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))
TIMEOUT_SECONDS = float(os.getenv("TIMEOUT_SECONDS", "60.0"))
DELAY_BETWEEN_BATCHES = float(os.getenv("DELAY_BETWEEN_BATCHES", "2.0"))

# GPTsAPI Base URL
GPTS_BASE_URL = os.getenv("GPTS_BASE_URL", "https://api.gptsapi.net/v1")

def create_client_with_proxy(proxy_url: Optional[str], api_key: str):
    """
    åˆ›å»ºå®¢æˆ·ç«¯ï¼Œå¦‚æœ PROXY_URL è®¾ç½®æœ‰æ•ˆåˆ™ä½¿ç”¨ä»£ç†ã€‚
    """
    http_client = None
    if proxy_url and proxy_url.strip().lower() not in {"", "none", "null", "no"}:
        if SyncProxyTransport is None:
            print("âš ï¸ ç¼ºå°‘ httpx-socks åº“ï¼Œæ— æ³•ä½¿ç”¨ä»£ç†ã€‚è¯·è¿è¡Œ: pip install httpx-socks")
        else:
            try:
                transport = SyncProxyTransport.from_url(proxy_url)
                http_client = httpx.Client(transport=transport, timeout=TIMEOUT_SECONDS)
                print(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy_url}")
            except Exception as e:
                print(f"âš ï¸ ä»£ç†é…ç½®é”™è¯¯ ({e}), å›é€€åˆ°ç›´è¿.")
                http_client = None

    client = OpenAI(api_key=api_key, http_client=http_client, base_url=GPTS_BASE_URL)
    print(f"ğŸ§© Base URL being used: {GPTS_BASE_URL}")
    return client, http_client


def embed_batch(texts: list[str], client: OpenAI) -> np.ndarray:
    # token è£å‰ª
    encoder = tiktoken.get_encoding("cl100k_base")
    max_tokens = 8000
    processed = []
    for txt in texts:
        ids = encoder.encode(txt)
        if len(ids) > max_tokens:
            ids = ids[:max_tokens]
            txt = encoder.decode(ids)
        processed.append(txt)

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = client.embeddings.create(
                model=EMBED_MODEL,
                input=processed,
                timeout=TIMEOUT_SECONDS
            )
            vecs = [item.embedding for item in resp.data]
            return np.array(vecs, dtype="float32")

        except openai.RateLimitError as e:
            retry_after = getattr(e, "retry_after", None)
            wait = retry_after if retry_after is not None else (2 ** attempt)
            print(f"âš ï¸ RateLimitError attempt {attempt}/{MAX_RETRIES}: {e}")
            print(f"   Retrying after {wait}s â€¦")
            time.sleep(wait)
            continue
        # ... (çœç•¥å…¶ä»–é”™è¯¯æ•è·ä»¥ä¿æŒç®€æ´)
        except Exception as e:
            print(f"âŒ é‡åˆ°é”™è¯¯ï¼Œå°è¯•æ¬¡æ•° {attempt}/{MAX_RETRIES}: {e}")
            if attempt == MAX_RETRIES:
                 raise
            time.sleep(5)
            continue

    raise RuntimeError(f"Embedding failed after {MAX_RETRIES} attempts")

def l2_normalize(x: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12
    return x / norms

def load_chunks(cleaned_dir: Path):
    texts = []
    metas = []
    jsonl_files = sorted(cleaned_dir.glob("*.jsonl"))
    if not jsonl_files:
        raise RuntimeError(f"No .jsonl files found in {cleaned_dir}")
    for jf in jsonl_files:
        print(f"ğŸ” Loading: {jf}")
        with open(jf, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    # å…³é”®ä¿®æ”¹ï¼šåµŒå…¥ augmented_text (æ‘˜è¦+æ­£æ–‡)
                    txt = obj.get("augmented_text", "").strip() 
                    if not txt:
                        continue
                    texts.append(txt)
                    # å…³é”®ä¿®æ”¹ï¼šå°†å®Œæ•´çš„ chunk å¯¹è±¡ä½œä¸ºå…ƒæ•°æ®å­˜å‚¨ï¼Œä»¥ä¾¿æ£€ç´¢æ—¶è®¿é—®æ‰€æœ‰ SAC ä¿¡æ¯
                    metas.append(obj) 
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ Skipping invalid JSON line in {jf}: {e}")
                    continue
    print(f"âœ… Loaded {len(texts)} SAC chunks")
    return texts, metas

def main():
    api_key = os.getenv("GPTSAPI_API_KEY")
    if not api_key:
        raise RuntimeError("è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ GPTSAPI_API_KEY")
    INDEX_DIR.mkdir(parents=True, exist_ok=True)

    client, http_client = create_client_with_proxy(PROXY_URL, api_key)

    try:
        texts, metas = load_chunks(CLEANED_DIR)
        if not texts:
            raise RuntimeError("No valid SAC text chunks found! è¯·ç¡®ä¿ 01_sac_pipeline.py å·²ç»æˆåŠŸè¿è¡Œã€‚")

        total = len(texts)
        batches = (total + BATCH_SIZE - 1) // BATCH_SIZE
        all_vecs = []
        print(f"ğŸš€ Starting embedding generation for {total} chunks...")
        for idx, i in enumerate(range(0, total, BATCH_SIZE)):
            batch = texts[i : i + BATCH_SIZE]
            print(f"â¡ï¸ Processing batch {idx+1}/{batches}, items {i+1}-{i+len(batch)} â€¦")
            vecs = embed_batch(batch, client)
            all_vecs.append(vecs)
            print(f"â± Sleeping {DELAY_BETWEEN_BATCHES}s between batches â€¦")
            time.sleep(DELAY_BETWEEN_BATCHES)

        X = np.vstack(all_vecs)
        print(f"âœ… Embedding matrix shape: {X.shape}")

        X = l2_normalize(X)

        dim = X.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(X)
        faiss.write_index(index, str(INDEX_PATH))
        print(f"ğŸ’¾ Saved index to: {INDEX_PATH}")

        with open(META_PATH, "w", encoding="utf-8") as f_out:
            for meta in metas:
                f_out.write(json.dumps(meta, ensure_ascii=False) + "\n")
        print(f"ğŸ’¾ Saved metadata to: {META_PATH}")

        print("ğŸ‰ All done! SAC Embedding + index built successfully.")
    finally:
        if http_client:
             http_client.close()

if __name__ == "__main__":
    main()