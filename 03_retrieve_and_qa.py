#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
03_retrieve_and_qa.py - æƒå¨æ€§æ„ŸçŸ¥æ£€ç´¢ä¸ RAG QA
åŠŸèƒ½ï¼š
 1. å®šä¹‰æƒå¨æ€§çŸ©é˜µ (Authority Matrix)ã€‚
 2. å®ç° score_chunk å¤šç»´æ‰“åˆ†å‡½æ•° (ç›¸ä¼¼åº¦ + æƒå¨æ€§ + è¯­è¨€å¥–åŠ±)ã€‚
 3. æ‰§è¡Œé‡æ’åºæ£€ç´¢ (Re-ranking) å¹¶ç”Ÿæˆé—®ç­”ã€‚
"""

import os
import json
import time
import numpy as np
import faiss
from pathlib import Path
from typing import Optional, List, Dict
from openai import OpenAI
import openai
import httpx
try:
    from httpx_socks import SyncProxyTransport
except ImportError:
    SyncProxyTransport = None

# ====== æ¨¡å‹ä¸é…ç½® ======
PROXY_URL = os.getenv("PROXY_URL", "")
GPTS_BASE_URL = os.getenv("GPTS_BASE_URL", "https://api.gptsapi.net/v1")

INDEX_PATH = Path("indexes/faiss.index")
META_PATH = Path("indexes/meta.jsonl")

EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-large")
QA_MODEL = os.getenv("QA_MODEL", "gpt-4o-mini") 
TOP_K = int(os.getenv("TOP_K", "5"))
MAX_EMBED_RETRIES = 5


# >>>>>>>>>>>>>>>>>> æƒå¨æ€§æ•°æ®æ¨¡å‹ <<<<<<<<<<<<<<<<<<

class AuthorityMatrix:
    """å®šä¹‰ä¸åŒæ³•åŸŸåœ¨ç‰¹å®šæ³•å¾‹ä¸»é¢˜ä¸‹çš„æƒå¨æ€§æƒé‡ (0.0 - 1.0)"""
    def __init__(self):
        # ç¤ºä¾‹æƒå¨æ€§çŸ©é˜µï¼šè¯·æ ¹æ®æ‚¨çš„ç ”ç©¶éœ€æ±‚å®šåˆ¶
        self.matrix = {
            "data_protection": { # ç¤ºä¾‹ï¼šGDPR
                "EU": 1.0, 
                "US": 0.6,
                "SG": 0.7
            },
            "contract_law": { # ç¤ºä¾‹ï¼šè‹±ç¾åˆ¤ä¾‹æ³•
                "US": 1.0,  
                "EU": 0.8,
                "SG": 0.9
            },
            "general": { # é»˜è®¤ fallback
                "EU": 0.8, "US": 0.8, "SG": 0.8
            }
        }
    
    def get_score(self, topic: str, jurisdiction: str) -> float:
        topic_scores = self.matrix.get(topic, self.matrix["general"])
        # å¦‚æœæ³•åŸŸä¸åœ¨ä¸»é¢˜åˆ—è¡¨ä¸­ï¼Œç»™ä¸€ä¸ªä¿å®ˆçš„åˆ†æ•° 0.5
        return topic_scores.get(jurisdiction, 0.5) 

def classify_query_topics(query: str) -> List[str]:
    """[Abstract] æ¨¡æ‹Ÿ LLM æˆ–å…³é”®è¯åŒ¹é…è¿›è¡Œä¸»é¢˜åˆ†ç±»"""
    if any(k in query.lower() for k in ["privacy", "gdpr", "data", "ä¸ªäººæ•°æ®"]):
        return ["data_protection"]
    if any(k in query.lower() for k in ["contract", "agreement", "åè®®", "è¿çº¦"]):
        return ["contract_law"]
    return ["general"]

# >>>>>>>>>>>>>>>>>> åˆå§‹åŒ–ä¸åŠ è½½ <<<<<<<<<<<<<<<<<<

def create_gptsapi_client(api_key: str, proxy_url: str = None):
    http_client = None
    if proxy_url and proxy_url.strip() != "" and SyncProxyTransport is not None:
        transport = SyncProxyTransport.from_url(proxy_url)
        http_client = httpx.Client(transport=transport, timeout=60.0)
        print(f"âœ… ä½¿ç”¨ä»£ç†ï¼š{proxy_url}")
    
    client = OpenAI(api_key=api_key, http_client=http_client, base_url=GPTS_BASE_URL)
    return client, http_client

def load_index_and_meta(index_path: Path, meta_path: Path):
    if not index_path.exists(): raise FileNotFoundError(f"æœªæ‰¾åˆ°å‘é‡ç´¢å¼•ï¼š{index_path}")
    if not meta_path.exists(): raise FileNotFoundError(f"æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶ï¼š{meta_path}")

    index = faiss.read_index(str(index_path))
    metas = []
    with open(meta_path, "r", encoding="utf-8") as f:
        for line in f:
            metas.append(json.loads(line))
    return index, metas

# >>>>>>>>>>>>>>>>>> æ ¸å¿ƒæ‰“åˆ†ä¸æ£€ç´¢é€»è¾‘ <<<<<<<<<<<<<<<<<<

def embed_query(query: str, client: OpenAI) -> np.ndarray:
    """å°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸º embedding å‘é‡ï¼Œå¹¶æ·»åŠ æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶"""
    for attempt in range(1, MAX_EMBED_RETRIES + 1):
        try:
            resp = client.embeddings.create(model=EMBED_MODEL, input=[query])
            vec = np.array(resp.data[0].embedding, dtype="float32")
            vec = vec / (np.linalg.norm(vec) + 1e-12) 
            return vec.reshape(1, -1)

        except openai.RateLimitError:
            wait_time = 2 ** attempt
            print(f"âš ï¸ Rate Limit Error (429). å°è¯• {attempt}/{MAX_EMBED_RETRIES}. ç­‰å¾… {wait_time}s...")
            time.sleep(wait_time)
            continue

        except Exception as e:
            if attempt == MAX_EMBED_RETRIES:
                raise e
            print(f"âŒ API æˆ–ç½‘ç»œé”™è¯¯. å°è¯• {attempt}/{MAX_EMBED_RETRIES}. ç­‰å¾… 5s...")
            time.sleep(5)
            continue

    raise RuntimeError("Embedding failed after maximum retries.")


def score_chunk(sim_score: float, 
                chunk_meta: Dict, 
                authority_matrix: AuthorityMatrix, 
                query_topics: List[str], 
                query_lang: str,
                # ä¼ å…¥å¯è°ƒæƒé‡
                alpha: float,  # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
                beta: float,   # æ³•åŸŸæƒå¨æ€§æƒé‡
                gamma: float   # è¯­è¨€åŒ¹é…å¥–åŠ±æƒé‡
               ) -> float:
    """å¤šç»´æ‰“åˆ†å‡½æ•°"""
    jurisdiction = chunk_meta.get("jurisdiction", "EU") 
    # è¯­è¨€å­—æ®µéœ€è¦å‡†ç¡®ï¼Œè¿™é‡Œç®€å•è®¾ä¸º 'en'ï¼Œæœªæ¥éœ€å®ç°è¯­è¨€æ£€æµ‹
    chunk_lang = chunk_meta.get("language", "en") 
    
    # 1. è·å–æ³•åŸŸæƒå¨åˆ†
    auth_scores = [authority_matrix.get_score(t, jurisdiction) for t in query_topics]
    authority_val = max(auth_scores) if auth_scores else 0.5
    
    # 2. è¯­è¨€åŒ¹é…å¥–åŠ± (Language Bonus)
    lang_bonus = 1.0 if chunk_lang == query_lang else 0.0
    
    # 3. ç»¼åˆåŠ æƒå…¬å¼ (Authority-Aware Score)
    final_score = (alpha * sim_score) + (beta * authority_val) + (gamma * lang_bonus)
    
    return final_score


def retrieve_with_authority(query: str, 
                            client: OpenAI, 
                            index: faiss.IndexFlatIP, 
                            metas: list, 
                            top_k: int, 
                            authority_matrix: AuthorityMatrix,
                            candidate_k: int = 50, 
                            alpha: float = 0.6, 
                            beta: float = 0.3, 
                            gamma: float = 0.1):
    """
    æƒå¨æ€§æ„ŸçŸ¥æ£€ç´¢æµç¨‹ï¼šæ£€ç´¢ Top-N -> é‡æ’åº -> è¿”å› Top-Kã€‚
    """
    query_vec = embed_query(query, client)

    # 2. å‘é‡æ£€ç´¢ (å¬å›é˜¶æ®µ): æ£€ç´¢ Top-N ä¸ªå€™é€‰
    D, I = index.search(query_vec, candidate_k) 
    
    # 3. è§£ææŸ¥è¯¢ä¸»é¢˜å’Œè¯­è¨€
    query_topics = classify_query_topics(query)
    query_lang = "en" # ç®€åŒ–ï¼šå‡è®¾æŸ¥è¯¢æ˜¯è‹±æ–‡

    scored_candidates = []
    # 4. é‡æ’åºé˜¶æ®µ
    for rank, idx in enumerate(I[0]):
        sim_score = float(D[0][rank]) 
        chunk_meta = metas[idx]      
        
        final_score = score_chunk(
            sim_score, chunk_meta, authority_matrix, query_topics, query_lang,
            alpha=alpha, beta=beta, gamma=gamma
        )
        
        # å‡†å¤‡è¾“å‡ºç»“æœ
        result_item = {
            "final_score": final_score,
            "original_sim": sim_score,
            "chunk_meta": chunk_meta # åŒ…å« jurisdiction, augmented_text, summary ç­‰
        }
        scored_candidates.append(result_item)
            
    # 5. æŒ‰æœ€ç»ˆå¾—åˆ†æ’åºå¹¶è¿”å› Top-K
    scored_candidates.sort(key=lambda x: x["final_score"], reverse=True)
    return scored_candidates[:top_k]


def generate_answer(question: str, retrieved_context: list, client: OpenAI):
    """ç”¨ GPTsAPI çš„å¯¹è¯æ¨¡å‹ç”Ÿæˆæ³•è§„é—®ç­”"""
    # ç¡®ä¿ LLM æ‹¿åˆ°çš„æ˜¯åŸå§‹æ–‡æœ¬ (text) è€Œä¸æ˜¯ Augmented Text
    context_text = "\n\n".join([f"- {item}" for item in retrieved_context])

    prompt = (
        "ä½ æ˜¯ä¸€åå›½é™…åŠ å¯†èµ„äº§åˆè§„é¡¾é—®ï¼Œè¯·åŸºäºä¸‹åˆ—æ³•è§„åŸæ–‡å›ç­”é—®é¢˜ã€‚\n"
        "åŠ¡å¿…å¼•ç”¨ç›¸å…³æ¡æ–‡ç¼–å·æˆ–æ ‡é¢˜ï¼Œçªå‡ºæ³•åŸŸã€‚\n\n"
        f"ã€æ³•è§„æ¡æ–‡æ‘˜è¦ã€‘:\n{context_text}\n\n"
        f"ã€ç”¨æˆ·é—®é¢˜ã€‘:\n{question}\n\n"
        "è¯·åœ¨å›ç­”æœ«å°¾ä»¥ã€å‡ºå¤„ï¼š[æ³•åŸŸ] æ–‡æ¡£åã€æ ¼å¼æ ‡æ˜å¼•ç”¨çš„ä¸»è¦æ¥æºã€‚"
    )

    resp = client.chat.completions.create(
        model=QA_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=512,
    )
    return resp.choices[0].message.content.strip()

# ====== ä¸»ç¨‹åº ======
def main():
    api_key = os.getenv("GPTSAPI_API_KEY")
    if not api_key: raise RuntimeError("è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ GPTSAPI_API_KEY")

    client, http_client = create_gptsapi_client(api_key, PROXY_URL)
    index, metas = load_index_and_meta(INDEX_PATH, META_PATH)
    print(f"âœ… Loaded FAISS index ({index.ntotal} vectors)")
    authority_matrix = AuthorityMatrix()

    while True:
        question = input("\nè¯·è¾“å…¥æ‚¨çš„æ³•è§„é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š\n> ").strip()
        if not question or question.lower() in ["exit", "quit"]: break

        try:
            top_k_results = retrieve_with_authority(
                question, client, index, metas, TOP_K, authority_matrix
            )
            
            # æå– LLM éœ€è¦çš„åŸå§‹æ–‡æœ¬å’Œæ³•åŸŸä¿¡æ¯
            context_for_llm = []
            for r in top_k_results:
                 meta = r['chunk_meta']
                 # LLM QA åªéœ€è¦åŸå§‹æ–‡æœ¬ (text)
                 context_for_llm.append(f"[{meta.get('jurisdiction')}/{meta.get('doc_id')}] - {meta.get('text')}") 

            # 2ï¸âƒ£ ç”Ÿæˆå›ç­”
            answer = generate_answer(question, context_for_llm, client)
            
        except Exception as e:
            print(f"âŒ æ— æ³•è§£ææˆ–ç”Ÿæˆå›ç­”ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ/ä»£ç†/å¯†é’¥é…ç½®ã€‚è¯¦ç»†ä¿¡æ¯ï¼š{e}")
            continue

        # 4ï¸âƒ£ ç»“æ„åŒ–è¾“å‡º
        print("\n=== ğŸ’¬ å›ç­” ===")
        print(answer)
        
        print("\n=== ğŸ“„ é‡æ’åºç»“æœ (Top 5) ===")
        for i, r in enumerate(top_k_results, start=1):
            meta = r['chunk_meta']
            print(f"  {i}. Final={r['final_score']:.4f} | Sim={r['original_sim']:.4f} | Juri={meta.get('jurisdiction')} | Doc={meta.get('doc_id')} | {meta.get('text')[:40]}...")

    if http_client:
        http_client.close()

if __name__ == "__main__":
    main()
