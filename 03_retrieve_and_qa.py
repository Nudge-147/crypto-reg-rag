#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
03_retrieve_and_qa.py - æƒå¨æ€§æ„ŸçŸ¥æ£€ç´¢ä¸ RAG QA
åŠŸèƒ½ï¼š
 1. å®šä¹‰æƒå¨æ€§çŸ©é˜µ (Authority Matrix)ã€‚
 2. å®ç° score_chunk å¤šç»´æ‰“åˆ†å‡½æ•° (ç›¸ä¼¼åº¦ + æƒå¨æ€§ + è¯­è¨€å¥–åŠ±)ã€‚
 3. æ‰§è¡Œé‡æ’åºæ£€ç´¢ (Re-ranking) å¹¶ç”Ÿæˆé—®ç­”ã€‚
"""

import os
import json
import time
import numpy as np
import faiss
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, List, Dict, Tuple, Union
from openai import OpenAI
import openai
import httpx
try:
    from httpx_socks import SyncProxyTransport
except ImportError:
    SyncProxyTransport = None

# ====== æ¨¡å‹ä¸é…ç½® ======
PROXY_URL = os.getenv("PROXY_URL", "")
GPTS_BASE_URL = os.getenv("GPTS_BASE_URL", "https://api.gptsapi.net/v1")

INDEX_PATH = Path("indexes/faiss.index")
META_PATH = Path("indexes/meta.jsonl")

EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-large")
QA_MODEL = os.getenv("QA_MODEL", "gpt-4o-mini") 
MAX_EMBED_RETRIES = 5
DEFAULT_TOP_K = 5
TOP_K = DEFAULT_TOP_K  # backward compatibility for benchmark scripts
MAX_TOP_K = 20
SUPPORTED_MODES = {"jurisdiction_specific", "deep_research"}


@dataclass
class QueryRequest:
    question: str
    target_jurisdictions: Optional[List[str]]
    mode: str
    top_k: int


@dataclass
class QueryResponse:
    answer: str
    retrieved_items: List[Dict]
    stats: Dict
    applied_jurisdictions: Optional[List[str]]
    warnings: List[str]


# >>>>>>>>>>>>>>>>>> æƒå¨æ€§æ•°æ®æ¨¡å‹ <<<<<<<<<<<<<<<<<<

class AuthorityMatrix:
    """å®šä¹‰ä¸åŒæ³•åŸŸåœ¨ç‰¹å®šæ³•å¾‹ä¸»é¢˜ä¸‹çš„æƒå¨æ€§æƒé‡ (0.0 - 1.0)"""
    def __init__(self):
        # ç¤ºä¾‹æƒå¨æ€§çŸ©é˜µï¼šè¯·æ ¹æ®æ‚¨çš„ç ”ç©¶éœ€æ±‚å®šåˆ¶
        self.matrix = {
            "data_protection": { # ç¤ºä¾‹ï¼šGDPR
                "EU": 1.0, 
                "US": 0.6,
                "SG": 0.7
            },
            "contract_law": { # ç¤ºä¾‹ï¼šè‹±ç¾åˆ¤ä¾‹æ³•
                "US": 1.0,  
                "EU": 0.8,
                "SG": 0.9
            },
            "general": { # é»˜è®¤ fallback
                "EU": 0.8, "US": 0.8, "SG": 0.8
            }
        }
    
    def get_score(self, topic: str, jurisdiction: str) -> float:
        topic_scores = self.matrix.get(topic, self.matrix["general"])
        # å¦‚æœæ³•åŸŸä¸åœ¨ä¸»é¢˜åˆ—è¡¨ä¸­ï¼Œç»™ä¸€ä¸ªä¿å®ˆçš„åˆ†æ•° 0.5
        return topic_scores.get(jurisdiction, 0.5) 

def classify_query_topics(query: str) -> List[str]:
    """[Abstract] æ¨¡æ‹Ÿ LLM æˆ–å…³é”®è¯åŒ¹é…è¿›è¡Œä¸»é¢˜åˆ†ç±»"""
    if any(k in query.lower() for k in ["privacy", "gdpr", "data", "ä¸ªäººæ•°æ®"]):
        return ["data_protection"]
    if any(k in query.lower() for k in ["contract", "agreement", "åè®®", "è¿çº¦"]):
        return ["contract_law"]
    return ["general"]

# >>>>>>>>>>>>>>>>>> åˆå§‹åŒ–ä¸åŠ è½½ <<<<<<<<<<<<<<<<<<

def create_gptsapi_client(api_key: str, proxy_url: str = None):
    http_client = None
    if proxy_url and proxy_url.strip() != "" and SyncProxyTransport is not None:
        transport = SyncProxyTransport.from_url(proxy_url)
        http_client = httpx.Client(transport=transport, timeout=60.0)
        print(f"âœ… ä½¿ç”¨ä»£ç†ï¼š{proxy_url}")
    
    client = OpenAI(api_key=api_key, http_client=http_client, base_url=GPTS_BASE_URL)
    return client, http_client

def load_index_and_meta(index_path: Path, meta_path: Path):
    if not index_path.exists(): raise FileNotFoundError(f"æœªæ‰¾åˆ°å‘é‡ç´¢å¼•ï¼š{index_path}")
    if not meta_path.exists(): raise FileNotFoundError(f"æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶ï¼š{meta_path}")

    index = faiss.read_index(str(index_path))
    metas = []
    with open(meta_path, "r", encoding="utf-8") as f:
        for line in f:
            metas.append(json.loads(line))
    return index, metas


def get_available_jurisdictions(metas: list) -> List[str]:
    vals = set()
    for m in metas:
        j = str(m.get("jurisdiction", "")).strip().upper()
        if j:
            vals.add(j)
    return sorted(vals)


def normalize_query_request(
    raw_request: Dict,
    available: List[str],
    strict: bool = True,
) -> Tuple[QueryRequest, Dict]:
    """
    ç»Ÿä¸€è¯·æ±‚åè®®:
      question: str (required)
      target_jurisdictions: list[str] | str | None
      mode: jurisdiction_specific | deep_research
      top_k: int (1..MAX_TOP_K)
    """
    warnings: List[str] = []
    invalid_jurisdictions: List[str] = []

    question = str(raw_request.get("question", "")).strip()
    if not question:
        raise ValueError("question ä¸èƒ½ä¸ºç©º")

    raw_mode = str(raw_request.get("mode", "jurisdiction_specific")).strip().lower()
    if raw_mode in SUPPORTED_MODES:
        mode = raw_mode
    elif strict:
        raise ValueError(
            f"mode éæ³•: {raw_mode}. æ”¯æŒå€¼: {sorted(SUPPORTED_MODES)}"
        )
    else:
        mode = "jurisdiction_specific"
        warnings.append(f"mode éæ³•ï¼Œå·²å›é€€ä¸º {mode}: {raw_mode}")

    raw_top_k = raw_request.get("top_k", DEFAULT_TOP_K)
    try:
        top_k = int(raw_top_k)
    except Exception:
        if strict:
            raise ValueError(f"top_k éæ³•: {raw_top_k}")
        top_k = DEFAULT_TOP_K
        warnings.append(f"top_k éæ³•ï¼Œå·²å›é€€ä¸º {DEFAULT_TOP_K}: {raw_top_k}")

    if top_k < 1 or top_k > MAX_TOP_K:
        if strict:
            raise ValueError(f"top_k è¶…å‡ºèŒƒå›´: {top_k}, å…è®¸èŒƒå›´ 1..{MAX_TOP_K}")
        clamped = max(1, min(top_k, MAX_TOP_K))
        warnings.append(f"top_k è¶…å‡ºèŒƒå›´ï¼Œå·²è°ƒæ•´ä¸º {clamped}: {top_k}")
        top_k = clamped

    raw_targets = raw_request.get("target_jurisdictions")
    if isinstance(raw_targets, str):
        available_set = set(available)
        dedup = []
        for token in raw_targets.split(","):
            v = token.strip().upper()
            if not v or v in dedup:
                continue
            dedup.append(v)
        targets = [v for v in dedup if v in available_set]
        invalid_jurisdictions = [v for v in dedup if v not in available_set]
    elif isinstance(raw_targets, list):
        available_set = set(available)
        dedup = []
        for item in raw_targets:
            v = str(item).strip().upper()
            if not v or v in dedup:
                continue
            dedup.append(v)
        targets = [v for v in dedup if v in available_set]
        invalid_jurisdictions = [v for v in dedup if v not in available_set]
    elif raw_targets is None:
        targets = None
    else:
        if strict:
            raise ValueError("target_jurisdictions å¿…é¡»ä¸ºå­—ç¬¦ä¸²ã€æ•°ç»„æˆ– null")
        targets = None
        warnings.append("target_jurisdictions ç±»å‹éæ³•ï¼Œå·²å¿½ç•¥")

    targets = targets if targets else None

    if invalid_jurisdictions:
        warnings.append(f"å¿½ç•¥æœªçŸ¥æ³•åŸŸ: {invalid_jurisdictions}")
        if strict and not targets:
            raise ValueError(
                f"target_jurisdictions å…¨éƒ¨æ— æ•ˆ: {invalid_jurisdictions}. "
                f"å¯ç”¨æ³•åŸŸ: {available}"
            )

    # deep_research æ¨¡å¼é»˜è®¤ä¸è¿‡æ»¤æ³•åŸŸ
    if mode == "deep_research" and targets:
        warnings.append("deep_research æ¨¡å¼ä¸‹å·²å¿½ç•¥ target_jurisdictions")
        targets = None

    request = QueryRequest(
        question=question,
        target_jurisdictions=targets,
        mode=mode,
        top_k=top_k,
    )
    validation = {
        "available_jurisdictions": available,
        "invalid_jurisdictions": invalid_jurisdictions,
        "warnings": warnings,
    }
    return request, validation

# >>>>>>>>>>>>>>>>>> æ ¸å¿ƒæ‰“åˆ†ä¸æ£€ç´¢é€»è¾‘ <<<<<<<<<<<<<<<<<<

def embed_query(query: str, client: OpenAI) -> np.ndarray:
    """å°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸º embedding å‘é‡ï¼Œå¹¶æ·»åŠ æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶"""
    for attempt in range(1, MAX_EMBED_RETRIES + 1):
        try:
            resp = client.embeddings.create(model=EMBED_MODEL, input=[query])
            vec = np.array(resp.data[0].embedding, dtype="float32")
            vec = vec / (np.linalg.norm(vec) + 1e-12) 
            return vec.reshape(1, -1)

        except openai.RateLimitError:
            wait_time = 2 ** attempt
            print(f"âš ï¸ Rate Limit Error (429). å°è¯• {attempt}/{MAX_EMBED_RETRIES}. ç­‰å¾… {wait_time}s...")
            time.sleep(wait_time)
            continue

        except Exception as e:
            if attempt == MAX_EMBED_RETRIES:
                raise e
            print(f"âŒ API æˆ–ç½‘ç»œé”™è¯¯. å°è¯• {attempt}/{MAX_EMBED_RETRIES}. ç­‰å¾… 5s...")
            time.sleep(5)
            continue

    raise RuntimeError("Embedding failed after maximum retries.")


def score_chunk(sim_score: float, 
                chunk_meta: Dict, 
                authority_matrix: AuthorityMatrix, 
                query_topics: List[str], 
                query_lang: str,
                # ä¼ å…¥å¯è°ƒæƒé‡
                alpha: float,  # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
                beta: float,   # æ³•åŸŸæƒå¨æ€§æƒé‡
                gamma: float   # è¯­è¨€åŒ¹é…å¥–åŠ±æƒé‡
               ) -> float:
    """å¤šç»´æ‰“åˆ†å‡½æ•°"""
    jurisdiction = chunk_meta.get("jurisdiction", "EU") 
    # è¯­è¨€å­—æ®µéœ€è¦å‡†ç¡®ï¼Œè¿™é‡Œç®€å•è®¾ä¸º 'en'ï¼Œæœªæ¥éœ€å®ç°è¯­è¨€æ£€æµ‹
    chunk_lang = chunk_meta.get("language", "en") 
    
    # 1. è·å–æ³•åŸŸæƒå¨åˆ†
    auth_scores = [authority_matrix.get_score(t, jurisdiction) for t in query_topics]
    authority_val = max(auth_scores) if auth_scores else 0.5
    
    # 2. è¯­è¨€åŒ¹é…å¥–åŠ± (Language Bonus)
    lang_bonus = 1.0 if chunk_lang == query_lang else 0.0
    
    # 3. ç»¼åˆåŠ æƒå…¬å¼ (Authority-Aware Score)
    final_score = (alpha * sim_score) + (beta * authority_val) + (gamma * lang_bonus)
    
    return final_score


def retrieve_with_authority(query: str, 
                            client: OpenAI, 
                            index: faiss.IndexFlatIP, 
                            metas: list, 
                            top_k: int, 
                            authority_matrix: AuthorityMatrix,
                            target_jurisdictions: Optional[List[str]] = None,
                            candidate_k: int = 50, 
                            alpha: float = 0.6, 
                            beta: float = 0.3, 
                            gamma: float = 0.1,
                            return_stats: bool = False) -> Union[List[Dict], Tuple[List[Dict], Dict]]:
    """
    æƒå¨æ€§æ„ŸçŸ¥æ£€ç´¢æµç¨‹ï¼šæ£€ç´¢ Top-N -> é‡æ’åº -> è¿”å› Top-Kã€‚
    """
    query_vec = embed_query(query, client)

    # 2. è§£ææŸ¥è¯¢ä¸»é¢˜å’Œè¯­è¨€
    query_topics = classify_query_topics(query)
    query_lang = "en" # ç®€åŒ–ï¼šå‡è®¾æŸ¥è¯¢æ˜¯è‹±æ–‡

    allowed_jurisdictions = set([j.upper() for j in (target_jurisdictions or [])])
    target_count = len(allowed_jurisdictions)
    needs_bucket_merge = target_count > 1
    min_per_jur = max(1, int(np.ceil(top_k / target_count))) if needs_bucket_merge else 0

    search_rounds = 0
    search_k = max(candidate_k, top_k)
    valid_candidates = 0
    filtered_candidates = 0
    scored_candidates: List[Dict] = []
    bucketed_candidates: Dict[str, List[Dict]] = {}

    while True:
        search_rounds += 1
        D, I = index.search(query_vec, min(search_k, index.ntotal))

        valid_candidates = 0
        filtered_candidates = 0
        scored_candidates = []
        bucketed_candidates = {}

        for rank, idx in enumerate(I[0]):
            if idx < 0 or idx >= len(metas):
                continue
            valid_candidates += 1

            sim_score = float(D[0][rank])
            chunk_meta = metas[idx]
            chunk_jur = str(chunk_meta.get("jurisdiction", "")).upper()
            if allowed_jurisdictions and chunk_jur not in allowed_jurisdictions:
                continue
            filtered_candidates += 1

            final_score = score_chunk(
                sim_score, chunk_meta, authority_matrix, query_topics, query_lang,
                alpha=alpha, beta=beta, gamma=gamma
            )

            result_item = {
                "final_score": final_score,
                "original_sim": sim_score,
                "chunk_meta": chunk_meta,
                "_meta_idx": int(idx),
            }
            scored_candidates.append(result_item)
            if needs_bucket_merge:
                bucketed_candidates.setdefault(chunk_jur, []).append(result_item)

        if not needs_bucket_merge:
            break

        bucket_counts = {
            j: len(bucketed_candidates.get(j, []))
            for j in sorted(allowed_jurisdictions)
        }
        enough_per_bucket = all(v >= min_per_jur for v in bucket_counts.values())
        reached_index_limit = search_k >= index.ntotal
        reached_round_limit = search_rounds >= 5
        if enough_per_bucket or reached_index_limit or reached_round_limit:
            break

        next_search_k = max(int(search_k * 1.8), search_k + (top_k * target_count * 4))
        search_k = min(next_search_k, index.ntotal)

    scored_candidates.sort(key=lambda x: x["final_score"], reverse=True)

    if not needs_bucket_merge:
        top_results = scored_candidates[:top_k]
    else:
        for jur_list in bucketed_candidates.values():
            jur_list.sort(key=lambda x: x["final_score"], reverse=True)

        selected = []
        selected_ids = set()

        for jur in sorted(allowed_jurisdictions):
            picks = 0
            for item in bucketed_candidates.get(jur, []):
                key = item["_meta_idx"]
                if key in selected_ids:
                    continue
                selected.append(item)
                selected_ids.add(key)
                picks += 1
                if picks >= min_per_jur or len(selected) >= top_k:
                    break
            if len(selected) >= top_k:
                break

        if len(selected) < top_k:
            for item in scored_candidates:
                key = item["_meta_idx"]
                if key in selected_ids:
                    continue
                selected.append(item)
                selected_ids.add(key)
                if len(selected) >= top_k:
                    break
        top_results = selected[:top_k]

    if return_stats:
        bucket_counts = {}
        if needs_bucket_merge:
            bucket_counts = {
                j: len(bucketed_candidates.get(j, []))
                for j in sorted(allowed_jurisdictions)
            }
        stats = {
            "candidate_k": candidate_k,
            "search_k_final": min(search_k, index.ntotal),
            "search_rounds": search_rounds,
            "retrieved_candidates": valid_candidates,
            "after_filter": filtered_candidates,
            "returned": len(top_results),
            "bucket_counts": bucket_counts,
        }
        return top_results, stats
    return top_results


def generate_answer(question: str, retrieved_context: list, client: OpenAI):
    """ç”¨ GPTsAPI çš„å¯¹è¯æ¨¡å‹ç”Ÿæˆæ³•è§„é—®ç­”"""
    # ç¡®ä¿ LLM æ‹¿åˆ°çš„æ˜¯åŸå§‹æ–‡æœ¬ (text) è€Œä¸æ˜¯ Augmented Text
    context_text = "\n\n".join([f"- {item}" for item in retrieved_context])

    prompt = (
        "ä½ æ˜¯ä¸€åå›½é™…åŠ å¯†èµ„äº§åˆè§„é¡¾é—®ï¼Œè¯·åŸºäºä¸‹åˆ—æ³•è§„åŸæ–‡å›ç­”é—®é¢˜ã€‚\n"
        "åŠ¡å¿…å¼•ç”¨ç›¸å…³æ¡æ–‡ç¼–å·æˆ–æ ‡é¢˜ï¼Œçªå‡ºæ³•åŸŸã€‚\n\n"
        f"ã€æ³•è§„æ¡æ–‡æ‘˜è¦ã€‘:\n{context_text}\n\n"
        f"ã€ç”¨æˆ·é—®é¢˜ã€‘:\n{question}\n\n"
        "è¯·åœ¨å›ç­”æœ«å°¾ä»¥ã€å‡ºå¤„ï¼š[æ³•åŸŸ] æ–‡æ¡£åã€æ ¼å¼æ ‡æ˜å¼•ç”¨çš„ä¸»è¦æ¥æºã€‚"
    )

    resp = client.chat.completions.create(
        model=QA_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=512,
    )
    return resp.choices[0].message.content.strip()


def execute_query(
    request: QueryRequest,
    client: OpenAI,
    index: faiss.IndexFlatIP,
    metas: list,
    authority_matrix: AuthorityMatrix,
) -> QueryResponse:
    top_k_results, retrieve_stats = retrieve_with_authority(
        request.question,
        client,
        index,
        metas,
        request.top_k,
        authority_matrix,
        target_jurisdictions=request.target_jurisdictions,
        return_stats=True,
    )

    warnings: List[str] = []
    if not top_k_results:
        warnings.append("å½“å‰æ£€ç´¢æ¡ä»¶ä¸‹æ— ç»“æœ")
        return QueryResponse(
            answer="",
            retrieved_items=[],
            stats=retrieve_stats,
            applied_jurisdictions=request.target_jurisdictions,
            warnings=warnings,
        )

    context_for_llm = []
    for r in top_k_results:
        meta = r["chunk_meta"]
        context_for_llm.append(
            f"[{meta.get('jurisdiction')}/{meta.get('doc_id')}] - {meta.get('text')}"
        )
    answer = generate_answer(request.question, context_for_llm, client)
    return QueryResponse(
        answer=answer,
        retrieved_items=top_k_results,
        stats=retrieve_stats,
        applied_jurisdictions=request.target_jurisdictions,
        warnings=warnings,
    )

# ====== ä¸»ç¨‹åº ======
def main():
    api_key = os.getenv("GPTSAPI_API_KEY")
    if not api_key: raise RuntimeError("è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ GPTSAPI_API_KEY")

    client, http_client = create_gptsapi_client(api_key, PROXY_URL)
    index, metas = load_index_and_meta(INDEX_PATH, META_PATH)
    print(f"âœ… Loaded FAISS index ({index.ntotal} vectors)")
    authority_matrix = AuthorityMatrix()
    available_jurisdictions = get_available_jurisdictions(metas)

    # CLI é€‚é…å±‚é»˜è®¤å€¼ï¼ˆæ ¸å¿ƒé€»è¾‘ä¸ç›´æ¥ä¾èµ–ç¯å¢ƒå˜é‡ï¼‰
    default_mode = os.getenv("MODE", "jurisdiction_specific")
    default_top_k = os.getenv("TOP_K", str(DEFAULT_TOP_K))
    default_targets_raw = os.getenv("TARGET_JURISDICTIONS", "")
    print(f"ğŸ“Œ å¯ç”¨æ³•åŸŸ: {available_jurisdictions}")

    # æ”¯æŒéäº¤äº’å¼è¿è¡Œï¼šä¼˜å…ˆè¯»å–ç¯å¢ƒå˜é‡ QUERY
    preset_query = os.getenv("QUERY", "").strip()
    while True:
        if preset_query:
            question = preset_query
            print(f"\n[non-interactive] QUERY = {question}")
        else:
            question = input("\nè¯·è¾“å…¥æ‚¨çš„æ³•è§„é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š\n> ").strip()
        if not question or question.lower() in ["exit", "quit"]:
            break

        try:
            raw_request = {
                "question": question,
                "target_jurisdictions": default_targets_raw,
                "mode": default_mode,
                "top_k": default_top_k,
            }
            request, validation = normalize_query_request(
                raw_request, available_jurisdictions, strict=False
            )
            response = execute_query(
                request,
                client,
                index,
                metas,
                authority_matrix,
            )
            if validation["warnings"]:
                print(f"âš ï¸ è¾“å…¥å‘Šè­¦: {validation['warnings']}")
            top_k_results = response.retrieved_items
            retrieve_stats = response.stats
            print(
                f"ğŸ¯ è¯·æ±‚å‚æ•°: mode={request.mode} | "
                f"target_jurisdictions={request.target_jurisdictions if request.target_jurisdictions else 'None'} | "
                f"top_k={request.top_k}"
            )
            print(
                "ğŸ” æ£€ç´¢ç»Ÿè®¡: "
                f"candidate_k={retrieve_stats['candidate_k']} | "
                f"valid={retrieve_stats['retrieved_candidates']} | "
                f"after_filter={retrieve_stats['after_filter']} | "
                f"top_k={retrieve_stats['returned']}"
            )

            if not top_k_results:
                print("âš ï¸ å½“å‰æ³•åŸŸè¿‡æ»¤ä¸‹æ— æ£€ç´¢ç»“æœã€‚è¯·è°ƒæ•´é—®é¢˜æˆ–æ”¾å®½æ³•åŸŸè¿‡æ»¤ã€‚")
                if preset_query:
                    break
                continue

        except Exception as e:
            print(f"âŒ æ— æ³•è§£ææˆ–ç”Ÿæˆå›ç­”ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ/ä»£ç†/å¯†é’¥é…ç½®ã€‚è¯¦ç»†ä¿¡æ¯ï¼š{e}")
            continue

        # 4ï¸âƒ£ ç»“æ„åŒ–è¾“å‡º
        print("\n=== ğŸ’¬ å›ç­” ===")
        print(response.answer)
        
        print(f"\n=== ğŸ“„ é‡æ’åºç»“æœ (Top {request.top_k}) ===")
        for i, r in enumerate(top_k_results, start=1):
            meta = r['chunk_meta']
            print(f"  {i}. Final={r['final_score']:.4f} | Sim={r['original_sim']:.4f} | Juri={meta.get('jurisdiction')} | Doc={meta.get('doc_id')} | {meta.get('text')[:40]}...")

        # éäº¤äº’æ¨¡å¼ä»…è·‘ä¸€æ¬¡
        if preset_query:
            break

    if http_client:
        http_client.close()

if __name__ == "__main__":
    main()
