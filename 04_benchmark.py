#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
04_benchmark.py - SAC/æƒå¨æ€§RAGç³»ç»Ÿè¯„ä¼°æ¡†æ¶
åŠŸèƒ½ï¼š
 1. åŠ è½½åŒ…å« 24 ä¸ªé—®é¢˜çš„ Golden Test Set (JSON)ã€‚
 2. å®ç°æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ (DRM, Jurisdiction Accuracy)ã€‚
 3. è¿è¡Œå¯¹æ¯”è¯„ä¼°ï¼šSAC (ä»…å‘é‡) vs SAC + Authority (æƒå¨æ€§é‡æ’åº)ã€‚
"""

import json
import numpy as np
import collections
import os
import argparse
import csv
from typing import List, Dict
from pathlib import Path
import statistics # ç”¨äºæ±‚å¹³å‡æ•°
import importlib.util
import sys

# æ ¸å¿ƒä¿®å¤: åŠ¨æ€å¯¼å…¥ 03 è„šæœ¬ä¸­å®šä¹‰çš„å…³é”®å‡½æ•°å’Œç±»ï¼ˆæ–‡ä»¶åä»¥æ•°å­—å¼€å¤´ä¸èƒ½ç”¨å¸¸è§„ importï¼‰
try:
    module_path = Path(__file__).parent / "03_retrieve_and_qa.py"
    spec = importlib.util.spec_from_file_location("retrieve_and_qa_03", module_path)
    retrieve_mod = importlib.util.module_from_spec(spec)
    sys.modules["retrieve_and_qa_03"] = retrieve_mod
    spec.loader.exec_module(retrieve_mod)  # type: ignore

    load_index_and_meta = retrieve_mod.load_index_and_meta
    retrieve_with_authority = retrieve_mod.retrieve_with_authority
    create_gptsapi_client = retrieve_mod.create_gptsapi_client
    TOP_K = getattr(retrieve_mod, "TOP_K", getattr(retrieve_mod, "DEFAULT_TOP_K", 5))
    AuthorityMatrix = retrieve_mod.AuthorityMatrix
    embed_query = retrieve_mod.embed_query
except Exception as e:
    print(f"âŒ å¯¼å…¥ 03_retrieve_and_qa.py å¤±è´¥ã€‚è¯·æ£€æŸ¥æ–‡ä»¶åå’Œè·¯å¾„ã€‚é”™è¯¯: {e}")
    exit(1)


# ====== 1. é…ç½®å’Œåˆå§‹åŒ– ======

# ç¯å¢ƒå˜é‡
API_KEY = os.getenv("GPTSAPI_API_KEY", "")
PROXY_URL = os.getenv("PROXY_URL", "")
client, http_client = create_gptsapi_client(API_KEY, PROXY_URL)

# åŠ è½½ç´¢å¼•
try:
    index, metas = load_index_and_meta(Path("indexes/faiss.index"), Path("indexes/meta.jsonl"))
    print(f"âœ… Loaded {index.ntotal} vectors for benchmarking.")
except Exception as e:
    print(f"âŒ FATAL: Index loading failed. Check 'indexes/' directory. Error: {e}")
    exit(1)


# æƒé‡é…ç½®
DEFAULT_ALPHA = 0.6  
DEFAULT_BETA = 0.3   
DEFAULT_GAMMA = 0.1  


# ====== 2. æ ¸å¿ƒï¼šå¼•ç”¨åˆ°æ–‡ä»¶åçš„æ˜ å°„è¡¨ ======
# å·¦è¾¹æ˜¯æµ‹è¯•é›†é‡Œçš„ citationï¼Œå³è¾¹æ˜¯ cleaned_sac/ ä¸‹çš„æ–‡ä»¶å ID

CITATION_TO_DOC_ID = {
    # --- US Manual Batch (10é¢˜) ---
    "SEC_v_Ripple_2024_Final_Judgment": "SEC_v_Ripple_2024_Final_Judgment",
    "SEC_v_Ripple_2025_06_26": "SEC_v_Ripple_2025_06_26",
    "US_PL119-27_GENIUS_Act_2025": "US_PL119-27_GENIUS_Act_2025",
    "US_BILL_S394_2025": "US_BILL_S394_2025",
    "CLARITY_Act_2025_RCP": "CLARITY_Act_2025_RCP",
    "SEC_v_Ripple_2025_Stipulation_PR47": "SEC_v_Ripple_2025_Stipulation_PR47",
    
    # --- ä¿ç•™ä¹‹å‰çš„éªŒè¯é›†æ˜ å°„ (å¯é€‰) ---
    "EU_MiCA_Citation": "EU_MiCA_2023",
    "SG_MAS_PSN02_Citation": "SG_MAS_PSN02_AML_CFT"
}

def resolve_doc_id(citation: str) -> str:
    """å°è¯•å°†å¼•ç”¨æ˜ å°„åˆ°ç³»ç»Ÿä¸­çš„ Doc ID"""
    for key, val in CITATION_TO_DOC_ID.items():
        if key in citation:
            return val
    # å›é€€ï¼šå¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨åŸå§‹ Citation çš„æ¸…ç†ç‰ˆæœ¬ä½œä¸º ID
    return citation.replace(" ", "_").replace(".", "").replace(",", "").upper()

# ====== 3. æµ‹è¯•é›†åŠ è½½ (æ¶‰åŠæµ‹è¯•é›†) ======

def load_golden_dataset(path: str = "tests/batches/test_set_B01_fixed.json") -> List[Dict]:
    """
    [æ¶‰åŠæµ‹è¯•é›†] åŠ è½½ JSON æµ‹è¯•é›†ã€‚
    æˆ‘ä»¬ä½¿ç”¨ fix_test_ids.py ç”Ÿæˆçš„ tests/batches/test_set_B01_fixed.json æ–‡ä»¶ã€‚
    """
    if not os.path.exists(path):
        print(f"âŒ Error: Test set file {path} not found. Please ensure it exists.")
        return []
    
    with open(path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)
    
    print(f"ğŸ“‚ Loaded {len(raw_data)} test cases for evaluation.")
    return raw_data

# ====== 4. æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ (æ¶‰åŠæµ‹è¯•é›†) ======

def compute_drm(retrieved_items: List[Dict], gold_sources: List[Dict]) -> float:
    """
    [æ¶‰åŠæµ‹è¯•é›†] è®¡ç®— Document-Level Retrieval Mismatch (DRM).
    DRM = (Top-K ä¸­æ¥è‡ªé”™è¯¯æ–‡æ¡£çš„ Chunk æ•°é‡) / K
    """
    k = len(retrieved_items)
    if k == 0: return 0.0
    
    # 1. ç¡®å®šæ­£ç¡®çš„ Doc ID åˆ—è¡¨
    correct_doc_ids = set()
    for source in gold_sources:
        # ä½¿ç”¨ Citation å­—æ®µæ˜ å°„åˆ°æˆ‘ä»¬çš„ç³»ç»Ÿ Doc ID
        mapped_id = resolve_doc_id(source.get("citation", ""))
        if mapped_id:
            correct_doc_ids.add(mapped_id)
            
    if not correct_doc_ids:
        print("âš ï¸ Case has no valid Gold Doc ID mapping, skipping DRM for this case.")
        return 0.0 # æ— æ³•è®¡ç®—

    # 2. è®¡ç®—ä¸åŒ¹é…æ•°é‡
    mismatches = 0
    for item in retrieved_items:
        retrieved_doc_id = item['chunk_meta']['doc_id']
        if retrieved_doc_id not in correct_doc_ids:
            mismatches += 1
            
    return mismatches / k

def compute_jurisdiction_accuracy(retrieved_items: List[Dict], target_jurisdictions: List[str]) -> float:
    """
    [æ¶‰åŠæµ‹è¯•é›†] æ³•åŸŸæ­£ç¡®æ€§ï¼šæ£€ç´¢ç»“æœä¸­æœ‰å¤šå°‘æ¯”ä¾‹æ¥è‡ªç›®æ ‡æ³•åŸŸã€‚
    """
    k = len(retrieved_items)
    if k == 0: return 0.0
    
    matches = 0
    for item in retrieved_items:
        if item['chunk_meta']['jurisdiction'] in target_jurisdictions:
            matches += 1
            
    return matches / k

# (Char P/R é€»è¾‘å› ä¾èµ–ç²¾ç¡®å­—èŠ‚åç§»é‡è€Œçœç•¥ï¼Œä¸“æ³¨äº DRM å’Œ Jur Acc)

# ====== 5. å˜ä½“è¿è¡Œå™¨ (Executor) ======

def run_system_variant(name: str, test_set: List[Dict], alpha, beta, gamma):
    """
    [æ¶‰åŠæµ‹è¯•é›†] éå†æµ‹è¯•é›†å¹¶è¿è¡ŒæŒ‡å®šå˜ä½“çš„è¯„ä¼°ã€‚
    """
    print(f"\n--- ğŸ§ª Running Variant: {name} (Î±={alpha}, Î²={beta}, Î³={gamma}) ---")
    all_results = collections.defaultdict(list)
    authority_matrix = AuthorityMatrix()
    case_rows = []

    for case in test_set:
        query = case["question_text"]
        
        # è¿è¡Œæ£€ç´¢
        try:
            # retrieve_with_authority éœ€è¦ AuthorityMatrix å®ä¾‹
            retrieved = retrieve_with_authority(
                query, client, index, metas, TOP_K, authority_matrix,
                alpha=alpha, beta=beta, gamma=gamma 
            )
        except Exception as e:
            print(f"âš ï¸ Retrieval error for q_id {case['id']}: {e}")
            continue
        
        # è®¡ç®—æŒ‡æ ‡ (æ‰€æœ‰æŒ‡æ ‡éƒ½ä½¿ç”¨äº†æµ‹è¯•é›†æ•°æ®)
        drm = compute_drm(retrieved, case.get("gold_sources", []))
        jur_acc = compute_jurisdiction_accuracy(retrieved, case.get("target_jurisdictions", []))

        # è°ƒè¯•è¾“å‡ºå½“å‰é—®é¢˜çš„æ£€ç´¢æƒ…å†µ
        print(f"\nğŸ” Debug Q: {case['question_text'][:30]}...")
        print(f"   Target Doc: {[s['citation'] for s in case.get('gold_sources', [])]}")
        print("   Retrieved Top 5:")
        for r in retrieved:
            meta = r['chunk_meta']
            print(f"     -> [{meta['jurisdiction']}] {meta['doc_id']} (Score: {r['final_score']:.4f})")
        
        all_results["drm"].append(drm)
        all_results["jur_acc"].append(jur_acc)
        top1_doc = retrieved[0]["chunk_meta"]["doc_id"] if retrieved else ""
        top1_jur = retrieved[0]["chunk_meta"]["jurisdiction"] if retrieved else ""
        case_rows.append(
            {
                "variant": name,
                "case_id": case.get("id", ""),
                "question_text": query,
                "target_jurisdictions": ",".join(case.get("target_jurisdictions", [])),
                "drm": f"{drm:.6f}",
                "jur_acc": f"{jur_acc:.6f}",
                "top1_doc_id": top1_doc,
                "top1_jurisdiction": top1_jur,
            }
        )
        
        # (Debug info)
        # print(f"  Q: {case['id']} | DRM: {drm:.4f} | J_Acc: {jur_acc:.4f} | Top Juri: {retrieved[0]['chunk_meta']['jurisdiction']}")


    # èšåˆç»“æœ
    avg_results = {k: statistics.mean(v) for k, v in all_results.items() if v}
    print(f"âœ… Aggregate Results for {name}:")
    print(json.dumps(avg_results, indent=2))
    return avg_results, case_rows


def write_csv(path: Path, rows: List[Dict], fieldnames: List[str]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)
    print(f"ğŸ’¾ CSV exported: {path}")

# ====== 6. ä¸»ç¨‹åºå…¥å£ ======

def main():
    parser = argparse.ArgumentParser(description="Run SAC/authority benchmark.")
    parser.add_argument(
        "--test-set",
        default="tests/manual/test_set_us_manual.json",
        help="Path to benchmark dataset JSON.",
    )
    parser.add_argument(
        "--out-csv",
        default="",
        help="Optional per-case CSV output path.",
    )
    parser.add_argument(
        "--out-summary-csv",
        default="",
        help="Optional summary CSV output path.",
    )
    args = parser.parse_args()

    # åŠ è½½æŒ‡å®šæµ‹è¯•é›†
    test_data = load_golden_dataset(args.test_set)
    
    if not test_data: return

    all_case_rows = []
    summary_rows = []

    # å˜ä½“ 1: RAG_SAC_Multi (SAC åŸºç¡€æ•ˆæœï¼Œä»…ä¾èµ–å‘é‡ç›¸ä¼¼åº¦)
    agg1, rows1 = run_system_variant(
        name="RAG_SAC_Multi (Vector Sim Only)", 
        test_set=test_data, 
        alpha=1.0, beta=0.0, gamma=0.0
    )
    all_case_rows.extend(rows1)
    summary_rows.append(
        {
            "variant": "RAG_SAC_Multi (Vector Sim Only)",
            "avg_drm": f"{agg1.get('drm', 0.0):.6f}",
            "avg_jur_acc": f"{agg1.get('jur_acc', 0.0):.6f}",
            "cases": len(rows1),
        }
    )

    # å˜ä½“ 2: RAG_SAC_Auth (å¯ç”¨æƒå¨æ€§é‡æ’åº)
    agg2, rows2 = run_system_variant(
        name="RAG_SAC_Auth (Full System)", 
        test_set=test_data, 
        alpha=DEFAULT_ALPHA, beta=DEFAULT_BETA, gamma=DEFAULT_GAMMA
    )
    all_case_rows.extend(rows2)
    summary_rows.append(
        {
            "variant": "RAG_SAC_Auth (Full System)",
            "avg_drm": f"{agg2.get('drm', 0.0):.6f}",
            "avg_jur_acc": f"{agg2.get('jur_acc', 0.0):.6f}",
            "cases": len(rows2),
        }
    )

    if args.out_csv:
        write_csv(
            Path(args.out_csv),
            all_case_rows,
            [
                "variant",
                "case_id",
                "question_text",
                "target_jurisdictions",
                "drm",
                "jur_acc",
                "top1_doc_id",
                "top1_jurisdiction",
            ],
        )
    if args.out_summary_csv:
        write_csv(
            Path(args.out_summary_csv),
            summary_rows,
            ["variant", "avg_drm", "avg_jur_acc", "cases"],
        )
    
    # ... (å¯ä»¥æ·»åŠ å…¶ä»–æƒé‡å˜ä½“è¿›è¡Œå¯¹æ¯”)
    print("\n--- Benchmark Complete ---")

if __name__ == "__main__":
    main()
